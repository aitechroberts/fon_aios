# setup_modular_pipeline.py
import asyncio
import os
import subprocess
from dotenv import load_dotenv

async def main():
    """Setup modular event-driven pipeline"""
    
    load_dotenv()
    
    print("üöÄ FON AIOS Modular Pipeline Setup")
    print("=" * 50)
    print("\nArchitecture:")
    print("  Ingestion ‚Üí JSON (by source) + Parquet ‚Üí Event ‚Üí Embeddings ‚Üí Azure Search")
    print("=" * 50)
    
    # Step 1: Check environment variables
    print("\n1Ô∏è‚É£ Checking environment variables...")
    required_vars = [
        "AZURE_STORAGE_CONNECTION_STRING",
        "AZURE_SEARCH_SERVICE_NAME", 
        "AZURE_SEARCH_ADMIN_KEY",
        "NEWSAPI_KEY",
        "OPENAI_API_KEY",
        "PREFECT_API_KEY",
        "PREFECT_API_URL"
    ]
    
    missing_vars = []
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print("‚ùå Missing environment variables:")
        for var in missing_vars:
            print(f"   - {var}")
        return
    else:
        print("‚úÖ All environment variables found")
    
    # Step 2: Login to Prefect Cloud
    print("\n2Ô∏è‚É£ Logging into Prefect Cloud...")
    try:
        subprocess.run(["prefect", "cloud", "login", "--key", os.getenv("PREFECT_API_KEY")], check=True)
        print("‚úÖ Logged into Prefect Cloud")
    except subprocess.CalledProcessError:
        print("‚ùå Failed to login to Prefect Cloud")
        return
    
    # Step 3: Create work pool
    print("\n3Ô∏è‚É£ Creating Prefect Managed work pool...")
    try:
        subprocess.run([
            "prefect", "work-pool", "create", 
            "managed-ingestion-pool", 
            "--type", "prefect:managed"
        ], check=True)
        print("‚úÖ Created managed work pool")
    except subprocess.CalledProcessError as e:
        if "already exists" in str(e):
            print("‚ÑπÔ∏è Work pool already exists")
        else:
            print("‚ùå Failed to create work pool")
            return
    
    # Step 4: Setup storage blocks
    print("\n4Ô∏è‚É£ Setting up storage blocks...")
    try:
        from blocks.setup_storage import setup_blocks
        await setup_blocks()
    except Exception as e:
        print(f"‚ùå Failed to setup blocks: {e}")
        return
    
    # Step 5: Test ingestion flow
    print("\n5Ô∏è‚É£ Testing news ingestion flow...")
    try:
        from flows.ingestion.news_ingestion_modular import news_ingestion_flow
        
        test_result = await news_ingestion_flow(
            query="DARPA",
            lookback_days=1
        )
        
        if test_result["status"] == "success":
            print(f"‚úÖ Ingestion test successful!")
            print(f"   - Articles: {test_result['articles_count']}")
            print(f"   - Sources: {', '.join(test_result['sources'][:3])}...")
            print(f"   - Parquet: {test_result['parquet_path']}")
            
            # Save parquet path for embeddings test
            test_parquet = test_result['parquet_path']
        else:
            print("‚ùå Ingestion test failed")
            return
    except Exception as e:
        print(f"‚ùå Ingestion test failed: {e}")
        return
    
    # Step 6: Test embeddings flow
    print("\n6Ô∏è‚É£ Testing embeddings flow...")
    try:
        from flows.processing.embeddings_flow import embeddings_processing_flow
        
        embed_result = await embeddings_processing_flow(
            parquet_path=test_parquet
        )
        
        if embed_result["status"] == "success":
            print(f"‚úÖ Embeddings test successful!")
            print(f"   - Processed: {embed_result['articles_processed']}")
            print(f"   - Indexed: {embed_result['articles_indexed']}")
        else:
            print("‚ùå Embeddings test failed")
            return
    except Exception as e:
        print(f"‚ùå Embeddings test failed: {e}")
        return
    
    # Step 7: Deploy flows with event triggers
    print("\n7Ô∏è‚É£ Creating deployments with event triggers...")
    try:
        from deployments.deploy_event_driven import deploy_event_driven_pipeline
        await deploy_event_driven_pipeline()
    except Exception as e:
        print(f"‚ùå Failed to create deployments: {e}")
        return
    
    print("\n‚úÖ Setup complete!")
    print("\nüìä What's been deployed:")
    print("1. Ingestion flows (3 sectors) - Store JSON by source + Parquet")
    print("2. Embeddings flow - Auto-triggers on ingestion completion")
    print("3. Event automation - Links the flows together")
    
    print("\nüîÑ Pipeline flow:")
    print("1. NewsAPI ‚Üí Group by source ‚Üí JSON files")
    print("2. All articles ‚Üí Parquet file (compressed)")
    print("3. Event trigger ‚Üí Read Parquet")
    print("4. Generate embeddings ‚Üí Index to Azure Search")
    
    print("\nüìà Benefits of this architecture:")
    print("‚Ä¢ Modular: Easy to add new sources")
    print("‚Ä¢ Efficient: Parquet is 5-10x smaller and faster")
    print("‚Ä¢ Observable: JSON files for debugging")
    print("‚Ä¢ Scalable: Event-driven processing")
    
    print("\nüí° Next steps:")
    print("1. Monitor at: https://app.prefect.cloud")
    print("2. Add more sources (RSS, Reddit, etc.)")
    print("3. Check source-specific JSONs for quality")
    print("4. Query Azure Search for embedded articles")

if __name__ == "__main__":
    asyncio.run(main())