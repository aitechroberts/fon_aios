{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b798508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsAPI.ai Key loaded: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Import\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "NEWSAPI_AI_KEY = os.getenv(\"NEWSAPI_AI_KEY\")\n",
    "NEWSAPI_AI_BASE = \"https://newsapi.ai/api/v1/article/getArticlesForTopicPage\"\n",
    "\n",
    "\n",
    "\n",
    "print(f\"NewsAPI.ai Key loaded: {'‚úÖ' if NEWSAPI_AI_KEY else '‚ùå'}\")\n",
    "\n",
    "if not NEWSAPI_AI_KEY:\n",
    "    print(\"\\nTo get NewsAPI.ai key:\")\n",
    "    print(\"1. Go to https://newsapi.ai/\")\n",
    "    print(\"2. Sign up for free account (10,000 articles/month)\")\n",
    "    print(\"3. Add to .env: NEWSAPI_AI_KEY=your_key_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2110c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Core Functions for Topic Retrieval\n",
    "def get_articles_from_topic(topic_uri, topic_name=\"Topic\", max_articles=100):\n",
    "    \"\"\"\n",
    "    Retrieve articles from a specific NewsAPI.ai topic\n",
    "    \n",
    "    Args:\n",
    "        topic_uri: The URI/ID of your topic from NewsAPI.ai\n",
    "        topic_name: Friendly name for logging\n",
    "        max_articles: Maximum articles to retrieve\n",
    "    \"\"\"\n",
    "    \n",
    "    if topic_uri.startswith(\"YOUR_\"):\n",
    "        print(f\"‚ö†Ô∏è  Please update {topic_name} topic URI in configuration\")\n",
    "        return []\n",
    "    \n",
    "    params = {\n",
    "\n",
    "        \"apiKey\": NEWSAPI_AI_KEY,\n",
    "        \"uri\": topic_uri,\n",
    "        \"infoArticleBodyLen\": -1,\n",
    "        \"resultType\": \"articles\",\n",
    "        \"articlesSortBy\": \"fq\",\n",
    "        }\n",
    "    #     # Language\n",
    "    #     \"lang\": \"eng\",\n",
    "        \n",
    "    #     # Include all relevant fields\n",
    "    #     \"includeArticleTitle\": True,\n",
    "    #     \"includeArticleBody\": True,\n",
    "    #     \"includeArticleBasicInfo\": True,\n",
    "    #     \"includeArticleCategories\": True,\n",
    "    #     \"includeSourceName\": True,\n",
    "    #     \"includeSourceLocation\": True,\n",
    "    #     \"includeArticleImage\": True,\n",
    "        \n",
    "    #     # Sorting and filtering\n",
    "    #     \"articlesSortBy\": \"date\",  # or \"rel\" for relevance\n",
    "    #     \"articlesSortByAsc\": False,  # Newest first\n",
    "    #     \"isDuplicateFilter\": \"skipDuplicates\",\n",
    "        \n",
    "    #     # Result settings\n",
    "    #     \"articlesCount\": max_articles,\n",
    "    #     \"resultType\": \"articles\"\n",
    "    # }\n",
    "    \n",
    "    print(f\"\\nüîç Fetching articles from topic: {topic_name}\")\n",
    "    print(f\"   Topic URI: {topic_uri}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(NEWSAPI_AI_BASE, params=params, timeout=30)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error {response.status_code}: {response.text[:200]}\")\n",
    "            return []\n",
    "        \n",
    "        data = response.json()\n",
    "        articles = data.get('articles', {}).get('results', [])\n",
    "        total = data.get('articles', {}).get('totalResults', 0)\n",
    "        \n",
    "        print(f\"‚úÖ Retrieved {len(articles)} articles (Total available: {total:,})\")\n",
    "        \n",
    "        return articles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_articles(articles, topic_name=\"Topic\"):\n",
    "    \"\"\"Analyze a set of articles for quality and relevance\"\"\"\n",
    "    \n",
    "    if not articles:\n",
    "        print(f\"No articles to analyze for {topic_name}\")\n",
    "        return None\n",
    "    \n",
    "    analysis = {\n",
    "        'topic': topic_name,\n",
    "        'total_articles': len(articles),\n",
    "        'sources': Counter(),\n",
    "        'dates': Counter(),\n",
    "        'categories': Counter(),\n",
    "        'relevance_scores': [],\n",
    "        'similarity_scores': [],\n",
    "        'body_lengths': [],\n",
    "        'has_image': 0\n",
    "    }\n",
    "    \n",
    "    for article in articles:\n",
    "        # Source analysis\n",
    "        source = article.get('source', {}).get('title', 'Unknown')\n",
    "        analysis['sources'][source] += 1\n",
    "        \n",
    "        # Date analysis\n",
    "        date_str = article.get('date', '')\n",
    "        if date_str:\n",
    "            analysis['dates'][date_str] += 1\n",
    "        \n",
    "        # Category analysis\n",
    "        categories = article.get('categories', [])\n",
    "        for cat in categories:\n",
    "            if cat.get('label'):\n",
    "                analysis['categories'][cat['label']] += 1\n",
    "        \n",
    "        # Score analysis\n",
    "        if 'relevance' in article:\n",
    "            analysis['relevance_scores'].append(article['relevance'])\n",
    "        if 'sim' in article:\n",
    "            analysis['similarity_scores'].append(article['sim'])\n",
    "        \n",
    "        # Content analysis\n",
    "        body = article.get('body', '')\n",
    "        if body:\n",
    "            analysis['body_lengths'].append(len(body))\n",
    "        \n",
    "        if article.get('image'):\n",
    "            analysis['has_image'] += 1\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def print_analysis(analysis):\n",
    "    \"\"\"Pretty print the analysis results\"\"\"\n",
    "    \n",
    "    if not analysis:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä Analysis for {analysis['topic']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Total articles: {analysis['total_articles']}\")\n",
    "    print(f\"Articles with images: {analysis['has_image']}\")\n",
    "    \n",
    "    # Source distribution\n",
    "    print(f\"\\nTop 10 Sources:\")\n",
    "    for source, count in analysis['sources'].most_common(10):\n",
    "        print(f\"  {source}: {count}\")\n",
    "    \n",
    "    # Date distribution\n",
    "    print(f\"\\nArticles by date:\")\n",
    "    for date, count in sorted(analysis['dates'].items(), reverse=True)[:7]:\n",
    "        print(f\"  {date}: {count}\")\n",
    "    \n",
    "    # Category distribution\n",
    "    if analysis['categories']:\n",
    "        print(f\"\\nTop 10 Categories:\")\n",
    "        for cat, count in analysis['categories'].most_common(10):\n",
    "            print(f\"  {cat}: {count}\")\n",
    "    \n",
    "    # Score statistics\n",
    "    if analysis['relevance_scores']:\n",
    "        scores = analysis['relevance_scores']\n",
    "        print(f\"\\nRelevance scores:\")\n",
    "        print(f\"  Min: {min(scores):.1f}, Max: {max(scores):.1f}, Avg: {sum(scores)/len(scores):.1f}\")\n",
    "    \n",
    "    if analysis['similarity_scores']:\n",
    "        scores = analysis['similarity_scores']\n",
    "        print(f\"\\nSimilarity scores:\")\n",
    "        print(f\"  Min: {min(scores):.1f}, Max: {max(scores):.1f}, Avg: {sum(scores)/len(scores):.1f}\")\n",
    "    \n",
    "    # Content statistics\n",
    "    if analysis['body_lengths']:\n",
    "        lengths = analysis['body_lengths']\n",
    "        print(f\"\\nArticle body lengths:\")\n",
    "        print(f\"  Min: {min(lengths):,} chars\")\n",
    "        print(f\"  Max: {max(lengths):,} chars\")\n",
    "        print(f\"  Avg: {sum(lengths)//len(lengths):,} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61a394d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Fetching articles from topic: Defense Technology\n",
      "   Topic URI: 3d9d3ac4-4ee8-479a-bb16-040d8f7f13bd\n",
      "‚úÖ Retrieved 100 articles (Total available: 5,791)\n",
      "\n",
      "üìä Analysis for Defense Technology\n",
      "============================================================\n",
      "Total articles: 100\n",
      "Articles with images: 94\n",
      "\n",
      "Top 10 Sources:\n",
      "  Army Recognition: 9\n",
      "  Defense News: 6\n",
      "  indiandefensenews.in: 6\n",
      "  Market Screener: 4\n",
      "  Aviation Week: 3\n",
      "  Visegr√°d Post: 3\n",
      "  Manufacturing.net: 2\n",
      "  machinist.in: 2\n",
      "  NewsDrum: 2\n",
      "  SpaceWar: 2\n",
      "\n",
      "Articles by date:\n",
      "  2025-08-06: 54\n",
      "  2025-08-05: 29\n",
      "  2025-08-04: 8\n",
      "  2025-08-03: 1\n",
      "  2025-08-02: 3\n",
      "  2025-08-01: 5\n",
      "\n",
      "Relevance scores:\n",
      "  Min: 80.0, Max: 130.0, Avg: 89.3\n",
      "\n",
      "Similarity scores:\n",
      "  Min: 0.0, Max: 0.9, Avg: 0.3\n",
      "\n",
      "Article body lengths:\n",
      "  Min: 394 chars\n",
      "  Max: 9,553 chars\n",
      "  Avg: 3,150 chars\n",
      "\n",
      "\n",
      "üìà Summary:\n",
      "============================================================\n",
      "Defense Technology: 100 articles\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test Individual Topics\n",
    "# Test each topic individually\n",
    "\n",
    "# topics = [\n",
    "#     (DEFENSE_TECH_TOPIC, \"Defense Technology\"),\n",
    "#     (DARPA_CONTRACTS_TOPIC, \"DARPA Contracts\"),\n",
    "#     (AUTONOMOUS_SYSTEMS_TOPIC, \"Autonomous Systems\"),\n",
    "#     (MAINTENANCE_TECH_TOPIC, \"Maintenance Technology\")\n",
    "# ]\n",
    "\n",
    "topics = [\n",
    "    (\"3d9d3ac4-4ee8-479a-bb16-040d8f7f13bd\", \"Defense Technology\"),]\n",
    "\n",
    "all_topic_articles = {}\n",
    "\n",
    "for topic_uri, topic_name in topics:\n",
    "    articles = get_articles_from_topic(topic_uri, topic_name, max_articles=50)\n",
    "    \n",
    "    if articles:\n",
    "        all_topic_articles[topic_name] = articles\n",
    "        analysis = analyze_articles(articles, topic_name)\n",
    "        print_analysis(analysis)\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No articles retrieved for {topic_name}\")\n",
    "\n",
    "print(f\"\\n\\nüìà Summary:\")\n",
    "print(\"=\"*60)\n",
    "for topic_name, articles in all_topic_articles.items():\n",
    "    print(f\"{topic_name}: {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12ffe95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∞ Sample Articles from Defense Technology\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Raven Space Systems Chooses Colorado for New Headquarters, Manufacturing Facility\n",
      "   Source: Manufacturing.net\n",
      "   Date: 2025-08-05T15:15:15Z\n",
      "   URL: https://www.manufacturing.net/additive-manufacturing/news/22947186/raven-space-systems-chooses-colorado-for-new-headquarters-manufacturing-facility\n",
      "   Relevance: 130\n",
      "   Similarity: 0\n",
      "   Preview: BROOMFIELD - Raven Space Systems, a 3D printing company that specializes in aerospace-grade composites, selected Broomfield, Colorado, for its new headquarters and manufacturing facility.\n",
      "\n",
      "The company...\n",
      "\n",
      "2. US Army readies to release new missile defense strategy soon\n",
      "   Source: Defense News\n",
      "   Date: 2025-08-05T23:41:15Z\n",
      "   URL: https://www.defensenews.com/land/2025/08/05/us-army-readies-to-release-new-missile-defense-strategy-soon/\n",
      "   Relevance: 110\n",
      "   Similarity: 0\n",
      "   Preview: HUNTSVILLE, Ala. -- The U.S. Army is about three months away from releasing its air and missile defense strategy for 2040, Lt. Gen. Sean Gainey, commander of the service's Space and Missile Defense Co...\n",
      "\n",
      "3. Finnish forces to test laser-guided artillery\n",
      "   Source: Defense News\n",
      "   Date: 2025-08-05T20:37:05Z\n",
      "   URL: https://www.defensenews.com/global/europe/2025/08/05/finnish-forces-to-test-laser-guided-artillery/\n",
      "   Relevance: 110\n",
      "   Similarity: 0\n",
      "   Preview: MILAN - Finland has planned a live-firing event to assess the potential of laser-guided artillery munitions and their compatibility with Finnish weapons, which could provide the Nordic country with st...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Display Sample Articles from Each Topic\n",
    "def display_sample_articles(articles, topic_name, num_samples=3):\n",
    "    \"\"\"Display sample articles from a topic\"\"\"\n",
    "    \n",
    "    print(f\"\\nüì∞ Sample Articles from {topic_name}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for i, article in enumerate(articles[:num_samples], 1):\n",
    "        print(f\"\\n{i}. {article.get('title', 'No title')}\")\n",
    "        print(f\"   Source: {article.get('source', {}).get('title', 'Unknown')}\")\n",
    "        print(f\"   Date: {article.get('dateTime', 'Unknown')}\")\n",
    "        print(f\"   URL: {article.get('url', 'No URL')}\")\n",
    "        \n",
    "        # Relevance and similarity\n",
    "        if 'relevance' in article:\n",
    "            print(f\"   Relevance: {article['relevance']}\")\n",
    "        if 'sim' in article:\n",
    "            print(f\"   Similarity: {article['sim']}\")\n",
    "        \n",
    "        # Categories\n",
    "        categories = article.get('categories', [])\n",
    "        if categories:\n",
    "            cat_labels = [cat.get('label', '') for cat in categories if cat.get('label')][:3]\n",
    "            if cat_labels:\n",
    "                print(f\"   Categories: {', '.join(cat_labels)}\")\n",
    "        \n",
    "        # Body preview\n",
    "        body = article.get('body', '')\n",
    "        if body:\n",
    "            preview = body[:200] + \"...\" if len(body) > 200 else body\n",
    "            print(f\"   Preview: {preview}\")\n",
    "\n",
    "# Display samples from each topic\n",
    "for topic_name, articles in all_topic_articles.items():\n",
    "    display_sample_articles(articles, topic_name, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c9e00ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Cross-Topic Analysis\n",
      "============================================================\n",
      "\n",
      "üì° Source Coverage by Topic:\n",
      "Defense Technology: 65 unique sources\n",
      "\n",
      "üìÖ Date Coverage:\n",
      "Defense Technology: 2025-08-01 to 2025-08-06 (6 days)\n",
      "\n",
      "üîç Checking for duplicate articles across topics...\n",
      "No duplicate articles found across topics\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Cross-Topic Analysis\n",
    "def cross_topic_analysis(all_articles_dict):\n",
    "    \"\"\"Compare articles across different topics\"\"\"\n",
    "    \n",
    "    print(\"\\nüîÑ Cross-Topic Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find overlapping sources\n",
    "    all_sources = {}\n",
    "    for topic_name, articles in all_articles_dict.items():\n",
    "        sources = set()\n",
    "        for article in articles:\n",
    "            source = article.get('source', {}).get('title', 'Unknown')\n",
    "            sources.add(source)\n",
    "        all_sources[topic_name] = sources\n",
    "    \n",
    "    print(\"\\nüì° Source Coverage by Topic:\")\n",
    "    for topic_name, sources in all_sources.items():\n",
    "        print(f\"{topic_name}: {len(sources)} unique sources\")\n",
    "    \n",
    "    # Find sources that appear in multiple topics\n",
    "    if len(all_sources) > 1:\n",
    "        print(\"\\nüîó Sources appearing in multiple topics:\")\n",
    "        source_topics = defaultdict(list)\n",
    "        for topic_name, sources in all_sources.items():\n",
    "            for source in sources:\n",
    "                source_topics[source].append(topic_name)\n",
    "        \n",
    "        multi_topic_sources = {s: topics for s, topics in source_topics.items() if len(topics) > 1}\n",
    "        for source, topics in sorted(multi_topic_sources.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "            print(f\"  {source}: {', '.join(topics)}\")\n",
    "    \n",
    "    # Date coverage analysis\n",
    "    print(\"\\nüìÖ Date Coverage:\")\n",
    "    for topic_name, articles in all_articles_dict.items():\n",
    "        dates = set()\n",
    "        for article in articles:\n",
    "            date = article.get('date', '')\n",
    "            if date:\n",
    "                dates.add(date)\n",
    "        if dates:\n",
    "            print(f\"{topic_name}: {min(dates)} to {max(dates)} ({len(dates)} days)\")\n",
    "    \n",
    "    # Find potentially duplicate articles across topics\n",
    "    print(\"\\nüîç Checking for duplicate articles across topics...\")\n",
    "    all_urls = defaultdict(list)\n",
    "    for topic_name, articles in all_articles_dict.items():\n",
    "        for article in articles:\n",
    "            url = article.get('url', '')\n",
    "            if url:\n",
    "                all_urls[url].append(topic_name)\n",
    "    \n",
    "    duplicates = {url: topics for url, topics in all_urls.items() if len(topics) > 1}\n",
    "    if duplicates:\n",
    "        print(f\"Found {len(duplicates)} articles appearing in multiple topics\")\n",
    "        for url, topics in list(duplicates.items())[:5]:\n",
    "            print(f\"  Article in: {', '.join(topics)}\")\n",
    "    else:\n",
    "        print(\"No duplicate articles found across topics\")\n",
    "\n",
    "# Run cross-topic analysis\n",
    "if all_topic_articles:\n",
    "    cross_topic_analysis(all_topic_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "211d1b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exported 100 articles to newsapi_ai_defense_technology_20250806.json\n",
      "\n",
      "‚úÖ Combined export: 100 articles to newsapi_ai_all_topics_20250806.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Export Results for Pipeline Testing\n",
    "def export_for_pipeline(articles, topic_name):\n",
    "    \"\"\"Export articles in a format suitable for your NiFi/Airflow pipeline\"\"\"\n",
    "    \n",
    "    export_data = []\n",
    "    \n",
    "    for article in articles:\n",
    "        # Transform to your pipeline's expected format\n",
    "        doc_id = article.get('uri', '').replace('/', '_') or f\"newsapi_ai_{hash(article.get('url', ''))}\"\n",
    "        \n",
    "        export_item = {\n",
    "            'doc_id': doc_id,\n",
    "            'title': article.get('title', ''),\n",
    "            'content': article.get('body', ''),\n",
    "            'description': article.get('body', '')[:500] if article.get('body') else '',\n",
    "            'link': article.get('url', ''),\n",
    "            'published_at': article.get('dateTime', ''),\n",
    "            'source': article.get('source', {}).get('title', 'Unknown'),\n",
    "            'source_type': 'news_article',\n",
    "            'topic': topic_name,\n",
    "            'relevance_score': article.get('relevance', 0),\n",
    "            'similarity_score': article.get('sim', 0),\n",
    "            'categories': [cat.get('label', '') for cat in article.get('categories', []) if cat.get('label')],\n",
    "            'ingestion_timestamp': datetime.utcnow().isoformat()\n",
    "        }\n",
    "        \n",
    "        export_data.append(export_item)\n",
    "    \n",
    "    return export_data\n",
    "\n",
    "# Export all topics\n",
    "all_exports = {}\n",
    "for topic_name, articles in all_topic_articles.items():\n",
    "    export_data = export_for_pipeline(articles, topic_name)\n",
    "    all_exports[topic_name] = export_data\n",
    "    \n",
    "    # Save to JSON file\n",
    "    filename = f\"newsapi_ai_{topic_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Exported {len(export_data)} articles to {filename}\")\n",
    "\n",
    "# Create a combined export\n",
    "if all_exports:\n",
    "    combined = []\n",
    "    for topic_name, data in all_exports.items():\n",
    "        combined.extend(data)\n",
    "    \n",
    "    combined_filename = f\"newsapi_ai_all_topics_{datetime.now().strftime('%Y%m%d')}.json\"\n",
    "    with open(combined_filename, 'w') as f:\n",
    "        json.dump(combined, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Combined export: {len(combined)} articles to {combined_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98dd1622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Article Structure Analysis\n",
      "==================================================\n",
      "Available fields in article:\n",
      "  - uri: str\n",
      "  - lang: str\n",
      "  - isDuplicate: bool\n",
      "  - date: str\n",
      "  - time: str\n",
      "  - dateTime: str\n",
      "  - dateTimePub: str\n",
      "  - dataType: str\n",
      "  - sim: int\n",
      "  - url: str\n",
      "  - title: str\n",
      "  - body: str\n",
      "  - source: dict\n",
      "  - authors: list\n",
      "  - image: str\n",
      "  - eventUri: NoneType\n",
      "  - sentiment: float\n",
      "  - wgt: int\n",
      "  - relevance: int\n",
      "\n",
      "Source fields:\n",
      "  - source.uri: str\n",
      "  - source.dataType: str\n",
      "  - source.title: str\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Analyze Response Structure\n",
    "if defense_articles:\n",
    "    print(\"\\nüìã Article Structure Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Look at first article structure\n",
    "    first_article = defense_articles[0]\n",
    "    print(\"Available fields in article:\")\n",
    "    for key in first_article.keys():\n",
    "        value_type = type(first_article[key]).__name__\n",
    "        print(f\"  - {key}: {value_type}\")\n",
    "    \n",
    "    # Check nested structures\n",
    "    if 'source' in first_article:\n",
    "        print(\"\\nSource fields:\")\n",
    "        for key in first_article['source'].keys():\n",
    "            print(f\"  - source.{key}: {type(first_article['source'][key]).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9498a4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Content Quality Analysis\n",
      "==================================================\n",
      "Articles with title: 100/100\n",
      "Articles with body: 100/100\n",
      "Articles with image: 100/100\n",
      "\n",
      "Body length stats:\n",
      "  Min: 57 chars\n",
      "  Max: 17535 chars\n",
      "  Avg: 2549 chars\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Content Quality Analysis\n",
    "print(\"\\nüìä Content Quality Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "content_stats = {\n",
    "    'has_body': 0,\n",
    "    'has_title': 0,\n",
    "    'has_image': 0,\n",
    "    'body_lengths': [],\n",
    "    'title_lengths': []\n",
    "}\n",
    "\n",
    "for article in defense_articles:\n",
    "    if article.get('title'):\n",
    "        content_stats['has_title'] += 1\n",
    "        content_stats['title_lengths'].append(len(article['title']))\n",
    "    \n",
    "    if article.get('body'):\n",
    "        content_stats['has_body'] += 1\n",
    "        content_stats['body_lengths'].append(len(article['body']))\n",
    "    \n",
    "    if article.get('image'):\n",
    "        content_stats['has_image'] += 1\n",
    "\n",
    "print(f\"Articles with title: {content_stats['has_title']}/{len(defense_articles)}\")\n",
    "print(f\"Articles with body: {content_stats['has_body']}/{len(defense_articles)}\")\n",
    "print(f\"Articles with image: {content_stats['has_image']}/{len(defense_articles)}\")\n",
    "\n",
    "if content_stats['body_lengths']:\n",
    "    print(f\"\\nBody length stats:\")\n",
    "    print(f\"  Min: {min(content_stats['body_lengths'])} chars\")\n",
    "    print(f\"  Max: {max(content_stats['body_lengths'])} chars\")\n",
    "    print(f\"  Avg: {sum(content_stats['body_lengths']) // len(content_stats['body_lengths'])} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7f2f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∞ Source Analysis\n",
      "==================================================\n",
      "Total unique sources: 71\n",
      "\n",
      "Top 15 sources:\n",
      "  The Manila times: 4 articles\n",
      "  Sports Illustrated: 3 articles\n",
      "  Weston Mercury: 3 articles\n",
      "  Colorado Springs Gazette: 3 articles\n",
      "  WGXA: 3 articles\n",
      "  Daily Voice: 3 articles\n",
      "  Daily Mail Online: 3 articles\n",
      "  odessa-journal.com: 3 articles\n",
      "  Daily Times: 2 articles\n",
      "  Kalkine Media: 2 articles\n",
      "  The Boston Globe: 2 articles\n",
      "  WXXV 25: 2 articles\n",
      "  Yorkregion.com: 2 articles\n",
      "  WXLV: 2 articles\n",
      "  GhanaWeb: 2 articles\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Source Analysis\n",
    "print(\"\\nüì∞ Source Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sources = Counter()\n",
    "source_countries = Counter()\n",
    "\n",
    "for article in defense_articles:\n",
    "    source = article.get('source', {})\n",
    "    source_name = source.get('title', 'Unknown')\n",
    "    sources[source_name] += 1\n",
    "    \n",
    "    # Check if location info is available\n",
    "    if 'location' in source:\n",
    "        country = source.get('location', {}).get('country', {}).get('label', {}).get('eng', 'Unknown')\n",
    "        source_countries[country] += 1\n",
    "\n",
    "print(f\"Total unique sources: {len(sources)}\")\n",
    "print(\"\\nTop 15 sources:\")\n",
    "for source, count in sources.most_common(15):\n",
    "    print(f\"  {source}: {count} articles\")\n",
    "\n",
    "if source_countries:\n",
    "    print(f\"\\nArticles by country:\")\n",
    "    for country, count in source_countries.most_common(10):\n",
    "        print(f\"  {country}: {count} articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "418ea5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ Date Distribution\n",
      "==================================================\n",
      "2025-08-05: 100 articles\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Date Distribution\n",
    "print(\"\\nüìÖ Date Distribution\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "date_distribution = Counter()\n",
    "\n",
    "for article in defense_articles:\n",
    "    date_str = article.get('dateTime', '')\n",
    "    if date_str:\n",
    "        try:\n",
    "            date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "            date_key = date.strftime('%Y-%m-%d')\n",
    "            date_distribution[date_key] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for date, count in sorted(date_distribution.items(), reverse=True):\n",
    "    print(f\"{date}: {count} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc89efcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sample Articles\n",
      "==================================================\n",
      "\n",
      "1. 'Clearly that's his opinion': Mike Johnson swatted down by Trump admin\n",
      "   Source: Raw Story\n",
      "   Date: 2025-08-05T20:10:16Z\n",
      "   URL: https://www.rawstory.com/mike-johnson-israel/\n",
      "   Preview: House Speaker Mike Johnson (R-LA) did not receive resounding support from the Trump administration over this week's comments regarding Israel and Gaza.\n",
      "\n",
      "During Tuesday's Pentagon news briefing, a repo...\n",
      "\n",
      "2. England face searching Ashes questions after India series thriller - Daily Times\n",
      "   Source: Daily Times\n",
      "   Date: 2025-08-05T20:10:10Z\n",
      "   URL: https://dailytimes.com.pk/1348832/england-face-searching-ashes-questions-after-india-series-thriller/\n",
      "   Preview: Their next major red-ball assignment is a five-match Ashes series away to arch-rivals Australia -- where England have gone 15 Tests without a win -- starting in November.\n",
      "\n",
      "Below AFP Sport looks at som...\n",
      "\n",
      "3. Sixers Big Man Named to National Team Roster\n",
      "   Source: Sports Illustrated\n",
      "   Date: 2025-08-05T20:10:03Z\n",
      "   URL: https://www.si.com/nba/76ers/news/sixers-big-man-named-to-national-team-roster-01k1vn33tjsn\n",
      "   Preview: Jul 10, 2025; Las Vegas, NV, USA; Philadelphia 76ers forward/center Adem Bona (30) looks on in the first quarter of their game against the San Antonio Spurs at Thomas & Mack Center. Mandatory Credit: ...\n",
      "\n",
      "4. L.A. agrees to pay $500,000 to reporters arrested at 2021 Echo Park protest\n",
      "   Source: Los Angeles Times\n",
      "   Date: 2025-08-05T20:10:00Z\n",
      "   URL: https://www.latimes.com/california/story/2025-08-05/echo-park-protest-arrest-settlement\n",
      "   Preview: The city of Los Angeles has tentatively agreed to pay $500,000 to two Knock LA journalists who claim their constitutional rights were violated when police arrested them while covering a protest four y...\n",
      "\n",
      "5. Avenio.ai Appoints Dr. Megha Vasavada as VP of Product Management to Accelerate GenAI in Clinical Trials & Operations\n",
      "   Source: Kalkine Media\n",
      "   Date: 2025-08-05T20:09:58Z\n",
      "   URL: https://kalkinemedia.com/news/world-news/avenioai-appoints-dr-megha-vasavada-as-vp-of-product-management-to-accelerate-genai-in-clinical-trials-operations\n",
      "   Preview: Kalkine Media dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut...\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Display Sample Articles\n",
    "print(\"\\nüìÑ Sample Articles\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, article in enumerate(defense_articles[:5], 1):\n",
    "    print(f\"\\n{i}. {article.get('title', 'No title')}\")\n",
    "    print(f\"   Source: {article.get('source', {}).get('title', 'Unknown')}\")\n",
    "    print(f\"   Date: {article.get('dateTime', 'Unknown')}\")\n",
    "    print(f\"   URL: {article.get('url', 'No URL')}\")\n",
    "    \n",
    "    body = article.get('body', '')\n",
    "    if body:\n",
    "        preview = body[:200] + \"...\" if len(body) > 200 else body\n",
    "        print(f\"   Preview: {preview}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fon_aios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
